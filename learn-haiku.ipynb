{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "import haiku as hk\n",
    "from einops import rearrange, reduce, repeat, einsum\n",
    "from functools import partial\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    head_dim: int\n",
    "    hidden_dim: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int\n",
    "    sliding_window: int\n",
    "    norm_eps: float\n",
    "    vocab_size: int\n",
    "    max_batch_size: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_seq = hk.PRNGSequence(42)\n",
    "nk = lambda : next(key_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = \"\"\"\n",
    "args = ModelArgs(\n",
    "    dim = 4096,\n",
    "    n_layers = 32,\n",
    "    head_dim = 128,\n",
    "    hidden_dim = 14336,\n",
    "    n_heads = 32,\n",
    "    n_kv_heads = 8,\n",
    "    norm_eps = 1e-05,\n",
    "    sliding_window = 4096,\n",
    "    vocab_size = 32000,\n",
    "    max_batch_size = 4\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax._src.lax.control_flow.loops import _interleave\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim, end, theta=10000.0):\n",
    "    freqs = 1.0 / (theta ** (jnp.arange(0, dim, 2)[:(dim // 2)] / dim))\n",
    "    t = jnp.arange(0, end)\n",
    "    freqs = jnp.outer(t, freqs).astype(jnp.float32)\n",
    "    return jax.lax.complex(jnp.cos(freqs), jnp.sin(freqs))\n",
    "\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    xq_ = xq.astype(jnp.float32)\n",
    "    xq_ = jax.lax.complex(xq_[..., ::2], xq_[..., 1::2]) * freqs_cis\n",
    "    xq_ = _interleave(jnp.real(xq_), jnp.imag(xq_), -1)\n",
    "    xk_ = xk.astype(jnp.float32)\n",
    "    xk_ = jax.lax.complex(xk_[..., ::2], xk_[..., 1::2]) * freqs_cis\n",
    "    xk_ = _interleave(jnp.real(xk_), jnp.imag(xk_), -1)\n",
    "    return xq_.astype(xq.dtype), xk_.astype(xk.dtype)\n",
    "\n",
    "\n",
    "def get_read_idxs(i, window):    \n",
    "    return jnp.arange(i - min(window, i + 1) + 1, i + 1) % window \n",
    "\n",
    "\n",
    "class Attention(hk.Module):\n",
    "    def __init__(self, args, name=None):\n",
    "        super().__init__(name)\n",
    "        self.args = args\n",
    "        self.scale = args.head_dim ** -0.5\n",
    "\n",
    "    def __call__(self, x, freqs_cis, positions, read_idxs, cache, mask, use_cache):\n",
    "        # assert False\n",
    "        args = self.args\n",
    "        wq = hk.Linear(args.n_heads * args.head_dim, with_bias=False, name='wq')\n",
    "        wk = hk.Linear(args.n_kv_heads * args.head_dim, with_bias=False, name='wk')\n",
    "        wv = hk.Linear(args.n_kv_heads * args.head_dim, with_bias=False, name='wv')\n",
    "        wo = hk.Linear(args.dim, with_bias=False, name='wo')\n",
    "\n",
    "        b, _, _ = x.shape\n",
    "        # reshape q into groups g for GQA\n",
    "        q = rearrange(wq(x), 'b l (g nkv dh) -> b g nkv l dh', dh=args.head_dim, nkv=args.n_kv_heads)\n",
    "        k = rearrange(wk(x), 'b l (nkv dh) -> b nkv l dh', dh=args.head_dim)\n",
    "        v = rearrange(wv(x), 'b l (nkv dh) -> b nkv l dh', dh=args.head_dim)\n",
    "        q, k = apply_rotary_emb(q, k, freqs_cis=freqs_cis)\n",
    "\n",
    "        # update cache\n",
    "        write_idxs = positions[-args.sliding_window:] % args.sliding_window\n",
    "        k_cache, v_cache = cache\n",
    "        k_cache = k_cache.at[:b, :, write_idxs].set(k[:, :, -args.sliding_window:])\n",
    "        v_cache = v_cache.at[:b, :, write_idxs].set(v[:, :, -args.sliding_window:])\n",
    "    \n",
    "        k_ = k_cache[:b, :, read_idxs]\n",
    "        v_ = v_cache[:b, :, read_idxs]\n",
    "\n",
    "        attention = partial(self.attention, q=q)\n",
    "        out = lax.cond(\n",
    "            use_cache, lambda : attention(k=k_, v=v_, mask=0), lambda : attention(k=k, v=v, mask=mask)\n",
    "        )\n",
    "\n",
    "        # out proj and updated cache\n",
    "        return wo(out), (k_cache, v_cache)\n",
    "\n",
    "    def attention(self, q, k, v, mask):\n",
    "        scores = einsum(q, k, 'b g h i k, b h j k -> b g h i j') * self.scale\n",
    "        sfmx = jax.nn.softmax(scores + mask, axis=-1).astype(q.dtype)\n",
    "        heads = einsum(sfmx, v, 'b g h i k, b h k j -> b g h i j')\n",
    "        return rearrange(heads, 'b g nkv l dh -> b l (g nkv dh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(hk.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, x):\n",
    "        args = self.args\n",
    "        w1 = hk.Linear(args.hidden_dim, with_bias=False, name='w1')\n",
    "        w2 = hk.Linear(args.dim, with_bias=False, name='w2')\n",
    "        w3 = hk.Linear(args.hidden_dim, with_bias=False, name='w3')\n",
    "        return w2(jax.nn.silu(w1(x)) * w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(hk.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(self, x, freqs_cis, positions, read_idxs, cache, mask, use_cache):\n",
    "        args = self.args\n",
    "        attention = Attention(args)\n",
    "        feed_forward = FeedForward(args)\n",
    "        attention_norm = hk.RMSNorm(-1, eps=args.norm_eps)\n",
    "        ffn_norm = hk.RMSNorm(-1, eps=args.norm_eps)\n",
    "\n",
    "        att, cache = attention(attention_norm(x), freqs_cis, positions, read_idxs, cache, mask, use_cache)\n",
    "        h = x + att\n",
    "        return h + feed_forward(ffn_norm(h)), cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(hk.Module):\n",
    "    def __init__(self, args, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.freqs_cis = precompute_freqs_cis(args.head_dim, max_seq_len)\n",
    "\n",
    "    def __call__(self, input_ids, positions, read_idxs, cache, use_cache=False):\n",
    "        args = self.args\n",
    "        norm = hk.RMSNorm(-1, eps=args.norm_eps)\n",
    "        tok_embeddings = hk.Embed(args.vocab_size, args.dim)\n",
    "        output = hk.Linear(args.vocab_size, with_bias=False)\n",
    "        layers = [TransformerBlock(args) for _ in range(args.n_layers)]\n",
    "\n",
    "        b, seq_len = input_ids.shape\n",
    "\n",
    "        h = tok_embeddings(input_ids)\n",
    "        freqs_cis = self.freqs_cis[positions]\n",
    "\n",
    "        tensor = jnp.ones((seq_len, seq_len), dtype=h.dtype)\n",
    "        mask = jnp.tril(tensor, k=0).astype(h.dtype)\n",
    "        mask = jnp.triu(mask, k=-self.args.sliding_window)\n",
    "        mask = jnp.log(mask)\n",
    "\n",
    "        k_cache, v_cache = cache\n",
    "\n",
    "        for i, layer in enumerate(layers):\n",
    "            kh, vh = k_cache[:b, i], v_cache[:b, i]\n",
    "            h, (kh, vh) = layer(h, freqs_cis, positions, read_idxs, (kh, vh), mask, use_cache)\n",
    "            k_cache = k_cache.at[:b, i].set(kh)\n",
    "            v_cache = v_cache.at[:b, i].set(vh)\n",
    "        \n",
    "        return output(norm(h)).astype(jnp.float32), (k_cache, v_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_args = ModelArgs(\n",
    "    dim=12 * 6,\n",
    "    n_layers=4,\n",
    "    head_dim=6,\n",
    "    hidden_dim=int(12 * 6 * 3.5),\n",
    "    n_heads=12,\n",
    "    n_kv_heads=4,\n",
    "    sliding_window=8,\n",
    "    norm_eps=1e-5,\n",
    "    vocab_size=10000,\n",
    "    max_batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.transform\n",
    "def f(input_ids, positions, read_idxs, cache, use_cache):\n",
    "    return Transformer(my_args, max_seq_len)(input_ids, positions, read_idxs, cache, use_cache)\n",
    "\n",
    "input_ids = jax.random.randint(nk(), (1, seq_len), 0, my_args.vocab_size)\n",
    "positions = jnp.arange(input_ids.shape[1])\n",
    "cache = (\n",
    "    jnp.zeros((my_args.max_batch_size, my_args.n_layers, my_args.n_kv_heads, my_args.sliding_window, my_args.head_dim)),\n",
    "    jnp.zeros((my_args.max_batch_size, my_args.n_layers, my_args.n_kv_heads, my_args.sliding_window, my_args.head_dim))\n",
    ")\n",
    "mask = 0\n",
    "use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TransformerBlock' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rd_idx \u001b[38;5;241m=\u001b[39m partial(get_read_idxs, window\u001b[38;5;241m=\u001b[39mmy_args\u001b[38;5;241m.\u001b[39msliding_window)\n\u001b[0;32m----> 2\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrd_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/environments/chat/lib/python3.10/site-packages/haiku/_src/transform.py:166\u001b[0m, in \u001b[0;36mwithout_state.<locals>.init_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m hk\u001b[38;5;241m.\u001b[39mMutableParams:\n\u001b[0;32m--> 166\u001b[0m   params, state \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m state:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m base\u001b[38;5;241m.\u001b[39mNonEmptyStateError(\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf your transformed function uses `hk.\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mget,set}_state` then use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`hk.transform_with_state`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/environments/chat/lib/python3.10/site-packages/haiku/_src/transform.py:422\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.init_fn\u001b[0;34m(rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base\u001b[38;5;241m.\u001b[39mnew_context(rng\u001b[38;5;241m=\u001b[39mrng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[1;32m    421\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mf\u001b[0;34m(input_ids, positions, read_idxs, cache, use_cache)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@hk\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(input_ids, positions, read_idxs, cache, use_cache):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/environments/chat/lib/python3.10/site-packages/haiku/_src/module.py:464\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    462\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 464\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/environments/chat/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/environments/chat/lib/python3.10/site-packages/haiku/_src/module.py:305\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 305\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    308\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    309\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method,\n\u001b[1;32m    310\u001b[0m                     orig_class\u001b[38;5;241m=\u001b[39morig_class)\n\u001b[1;32m    311\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, input_ids, positions, read_idxs, cache, use_cache)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layers):\n\u001b[1;32m     27\u001b[0m     kh, vh \u001b[38;5;241m=\u001b[39m k_cache[:b, i], v_cache[:b, i]\n\u001b[0;32m---> 28\u001b[0m     h, (kh, vh) \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     k_cache \u001b[38;5;241m=\u001b[39m k_cache\u001b[38;5;241m.\u001b[39mat[:b, i]\u001b[38;5;241m.\u001b[39mset(kh)\n\u001b[1;32m     30\u001b[0m     v_cache \u001b[38;5;241m=\u001b[39m v_cache\u001b[38;5;241m.\u001b[39mat[:b, i]\u001b[38;5;241m.\u001b[39mset(vh)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TransformerBlock' object is not callable"
     ]
    }
   ],
   "source": [
    "rd_idx = partial(get_read_idxs, window=my_args.sliding_window)\n",
    "params = f.init(nk(), input_ids, positions, rd_idx(seq_len - 1), cache, use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f.apply\n",
    "jit_fn = jax.jit(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit out, (kh, vh) = fn(params, nk(), input_ids, positions, rd_idx(seq_len - 1), cache, use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit out, (kh, vh) = jit_fn(params, nk(), input_ids, positions, rd_idx(seq_len - 1), cache, use_cache)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
